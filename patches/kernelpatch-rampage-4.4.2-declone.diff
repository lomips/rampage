diff --git a/include/linux/mm.h b/include/linux/mm.h
index 00bad77..03ab2bc 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -519,6 +519,17 @@ void put_pages_list(struct list_head *pages);
 void split_page(struct page *page, unsigned int order);
 int split_free_page(struct page *page);
 
+/* moved from mm/internal.h for RAMPAGE -ik */
+#ifdef CONFIG_MEMORY_FAILURE
+extern bool is_free_buddy_page(struct page *page);
+#endif
+
+/* wrapper functions for RAMPAGE (original is static inline) -ik */
+unsigned long mm_page_order(struct page *page);
+void mm_rmv_page_order(struct page *page);
+void mm_buddy_expand(struct zone *zone, struct page *page, int low, int high,
+		     struct free_area *area, int migratetype);
+
 /*
  * Compound pages have a destructor function.  Provide a
  * prototype for that function and accessor functions.
diff --git a/kernel/resource.c b/kernel/resource.c
index f150dbb..66614fc 100644
--- a/kernel/resource.c
+++ b/kernel/resource.c
@@ -475,6 +475,7 @@ int walk_system_ram_range(unsigned long start_pfn, unsigned long nr_pages,
 	}
 	return ret;
 }
+EXPORT_SYMBOL_GPL(walk_system_ram_range);
 
 #endif
 
diff --git a/mm/internal.h b/mm/internal.h
index 38e24b8..cabc092 100644
--- a/mm/internal.h
+++ b/mm/internal.h
@@ -178,9 +178,6 @@ extern int __isolate_free_page(struct page *page, unsigned int order);
 extern void __free_pages_bootmem(struct page *page, unsigned long pfn,
 					unsigned int order);
 extern void prep_compound_page(struct page *page, unsigned int order);
-#ifdef CONFIG_MEMORY_FAILURE
-extern bool is_free_buddy_page(struct page *page);
-#endif
 extern int user_min_free_kbytes;
 
 #if defined CONFIG_COMPACTION || defined CONFIG_CMA
diff --git a/mm/memory-failure.c b/mm/memory-failure.c
index 8424b64..10d238b 100644
--- a/mm/memory-failure.c
+++ b/mm/memory-failure.c
@@ -65,6 +65,7 @@ int sysctl_memory_failure_early_kill __read_mostly = 0;
 int sysctl_memory_failure_recovery __read_mostly = 1;
 
 atomic_long_t num_poisoned_pages __read_mostly = ATOMIC_LONG_INIT(0);
+EXPORT_SYMBOL_GPL(num_poisoned_pages);
 
 #if defined(CONFIG_HWPOISON_INJECT) || defined(CONFIG_HWPOISON_INJECT_MODULE)
 
@@ -220,10 +221,13 @@ static int kill_proc(struct task_struct *t, unsigned long addr, int trapno,
  */
 void shake_page(struct page *p, int access)
 {
+	pr_alert("*** shake_page %lu\n",page_to_pfn(p));
 	if (!PageSlab(p)) {
+		pr_alert("**** calling lru_add_drain_all()\n");
 		lru_add_drain_all();
 		if (PageLRU(p))
 			return;
+		pr_alert("**** calling drain_all_pages()\n");
 		drain_all_pages(page_zone(p));
 		if (PageLRU(p) || is_free_buddy_page(p))
 			return;
@@ -1781,3 +1785,4 @@ int soft_offline_page(struct page *page, int flags)
 	}
 	return ret;
 }
+EXPORT_SYMBOL_GPL(soft_offline_page);
diff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c
index a042a9d..03523b04 100644
--- a/mm/memory_hotplug.c
+++ b/mm/memory_hotplug.c
@@ -118,6 +118,7 @@ void mem_hotplug_begin(void)
 		schedule();
 	}
 }
+EXPORT_SYMBOL_GPL(mem_hotplug_begin);
 
 void mem_hotplug_done(void)
 {
@@ -125,6 +126,7 @@ void mem_hotplug_done(void)
 	mutex_unlock(&mem_hotplug.lock);
 	memhp_lock_release();
 }
+EXPORT_SYMBOL_GPL(mem_hotplug_done);
 
 /* add this memory to iomem resource */
 static struct resource *register_memory_resource(u64 start, u64 size)
@@ -1074,6 +1076,7 @@ int __ref online_pages(unsigned long pfn, unsigned long nr_pages, int online_typ
 		memory_notify(MEM_ONLINE, &arg);
 	return 0;
 }
+EXPORT_SYMBOL(online_pages);
 #endif /* CONFIG_MEMORY_HOTPLUG_SPARSE */
 
 static void reset_node_present_pages(pg_data_t *pgdat)
@@ -1473,7 +1476,7 @@ do_migrate_range(unsigned long start_pfn, unsigned long end_pfn)
 					    page_is_file_cache(page));
 
 		} else {
-#ifdef CONFIG_DEBUG_VM
+#if 0 && defined(CONFIG_DEBUG_VM)
 			printk(KERN_ALERT "removing pfn %lx from LRU failed\n",
 			       pfn);
 			dump_page(page, "failed to remove from LRU");
@@ -1852,6 +1855,7 @@ int offline_pages(unsigned long start_pfn, unsigned long nr_pages)
 {
 	return __offline_pages(start_pfn, start_pfn + nr_pages, 120 * HZ);
 }
+EXPORT_SYMBOL(offline_pages);
 #endif /* CONFIG_MEMORY_HOTREMOVE */
 
 /**
diff --git a/mm/migrate.c b/mm/migrate.c
index 7890d0b..b34364f 100644
--- a/mm/migrate.c
+++ b/mm/migrate.c
@@ -1185,6 +1185,7 @@ out:
 
 	return rc;
 }
+EXPORT_SYMBOL_GPL(migrate_pages);
 
 #ifdef CONFIG_NUMA
 /*
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 9d666df..6781880 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -1328,6 +1328,14 @@ static inline void expand(struct zone *zone, struct page *page,
 	}
 }
 
+/* wrapper function for RAMPAGE -ik */
+void mm_buddy_expand(struct zone *zone, struct page *page, int low, int high,
+		     struct free_area *area, int migratetype)
+{
+	expand(zone, page, low, high, area, migratetype);
+}
+EXPORT_SYMBOL_GPL(mm_buddy_expand);
+
 /*
  * This page is about to be returned from the page allocator
  */
@@ -1980,6 +1988,7 @@ void drain_all_pages(struct zone *zone)
 	on_each_cpu_mask(&cpus_with_pcps, (smp_call_func_t) drain_local_pages,
 								zone, 1);
 }
+EXPORT_SYMBOL_GPL(drain_all_pages);
 
 #ifdef CONFIG_HIBERNATION
 
@@ -3577,6 +3586,7 @@ unsigned long nr_free_pagecache_pages(void)
 {
 	return nr_free_zone_pages(gfp_zone(GFP_HIGHUSER_MOVABLE));
 }
+EXPORT_SYMBOL_GPL(nr_free_pagecache_pages);
 
 static inline void show_node(struct zone *zone)
 {
@@ -6081,6 +6091,7 @@ void setup_per_zone_wmarks(void)
 	__setup_per_zone_wmarks();
 	mutex_unlock(&zonelists_mutex);
 }
+EXPORT_SYMBOL_GPL(setup_per_zone_wmarks);
 
 /*
  * The inactive anon list should be small enough that the VM never has to
@@ -6103,7 +6114,7 @@ void setup_per_zone_wmarks(void)
  *    1TB     101        10GB
  *   10TB     320        32GB
  */
-static void __meminit calculate_zone_inactive_ratio(struct zone *zone)
+void __meminit calculate_zone_inactive_ratio(struct zone *zone)
 {
 	unsigned int gb, ratio;
 
@@ -6116,6 +6127,7 @@ static void __meminit calculate_zone_inactive_ratio(struct zone *zone)
 
 	zone->inactive_ratio = ratio;
 }
+EXPORT_SYMBOL_GPL(calculate_zone_inactive_ratio);
 
 static void __meminit setup_per_zone_inactive_ratio(void)
 {
@@ -6402,8 +6414,7 @@ void *__init alloc_large_system_hash(const char *tablename,
 }
 
 /* Return a pointer to the bitmap storing bits affecting a block of pages */
-static inline unsigned long *get_pageblock_bitmap(struct zone *zone,
-							unsigned long pfn)
+unsigned long *get_pageblock_bitmap(struct zone *zone, unsigned long pfn)
 {
 #ifdef CONFIG_SPARSEMEM
 	return __pfn_to_section(pfn)->pageblock_flags;
@@ -6411,6 +6422,7 @@ static inline unsigned long *get_pageblock_bitmap(struct zone *zone,
 	return zone->pageblock_flags;
 #endif /* CONFIG_SPARSEMEM */
 }
+EXPORT_SYMBOL_GPL(get_pageblock_bitmap);
 
 static inline int pfn_to_bitidx(struct zone *zone, unsigned long pfn)
 {
@@ -6451,6 +6463,7 @@ unsigned long get_pfnblock_flags_mask(struct page *page, unsigned long pfn,
 	bitidx += end_bitidx;
 	return (word >> (BITS_PER_LONG - bitidx - 1)) & mask;
 }
+EXPORT_SYMBOL(get_pfnblock_flags_mask);
 
 /**
  * set_pfnblock_flags_mask - Set the requested group of flags for a pageblock_nr_pages block of pages
@@ -6493,6 +6506,25 @@ void set_pfnblock_flags_mask(struct page *page, unsigned long flags,
 	}
 }
 
+/* wrapper function for RAMPAGE -ik */
+void mm_rmv_page_order(struct page *page)
+{
+	rmv_page_order(page);
+}
+EXPORT_SYMBOL_GPL(mm_rmv_page_order);
+
+/* 
+ * wrapper function for RAMPAGE -ik
+ * note: moving page_order to mm.h creates a
+ * conflict in kernel/events/internal.h
+ */
+unsigned long mm_page_order(struct page *page)
+{
+	return page_order(page);
+}
+EXPORT_SYMBOL_GPL(mm_page_order);
+
+
 /*
  * This function checks whether pageblock includes unmovable pages or not.
  * If @count is not zero, it is okay to include less @count unmovable pages
@@ -6887,6 +6919,7 @@ __offline_isolated_pages(unsigned long start_pfn, unsigned long end_pfn)
 	}
 	spin_unlock_irqrestore(&zone->lock, flags);
 }
+EXPORT_SYMBOL_GPL(__offline_isolated_pages);
 #endif
 
 #ifdef CONFIG_MEMORY_FAILURE
@@ -6908,4 +6941,5 @@ bool is_free_buddy_page(struct page *page)
 
 	return order < MAX_ORDER;
 }
+EXPORT_SYMBOL_GPL(is_free_buddy_page);
 #endif
diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 4568fd5..998a38d 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -9,7 +9,7 @@
 #include <linux/hugetlb.h>
 #include "internal.h"
 
-static int set_migratetype_isolate(struct page *page,
+int set_migratetype_isolate(struct page *page,
 				bool skip_hwpoisoned_pages)
 {
 	struct zone *zone;
@@ -72,8 +72,9 @@ out:
 		drain_all_pages(zone);
 	return ret;
 }
+EXPORT_SYMBOL(set_migratetype_isolate);
 
-static void unset_migratetype_isolate(struct page *page, unsigned migratetype)
+void unset_migratetype_isolate(struct page *page, unsigned migratetype)
 {
 	struct zone *zone;
 	unsigned long flags, nr_pages;
@@ -128,6 +129,7 @@ out:
 	if (isolated_page)
 		__free_pages(isolated_page, order);
 }
+EXPORT_SYMBOL(unset_migratetype_isolate);
 
 static inline struct page *
 __first_valid_page(unsigned long pfn, unsigned long nr_pages)
@@ -184,6 +186,7 @@ undo:
 
 	return -EBUSY;
 }
+EXPORT_SYMBOL(start_isolate_page_range);
 
 /*
  * Make isolated pages available again.
@@ -205,6 +208,8 @@ int undo_isolate_page_range(unsigned long start_pfn, unsigned long end_pfn,
 	}
 	return 0;
 }
+EXPORT_SYMBOL(undo_isolate_page_range);
+
 /*
  * Test all pages in the range is free(means isolated) or not.
  * all pages in [start_pfn...end_pfn) must be in the same zone.
@@ -271,6 +276,7 @@ int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn,
 	spin_unlock_irqrestore(&zone->lock, flags);
 	return ret ? 0 : -EBUSY;
 }
+EXPORT_SYMBOL(test_pages_isolated);
 
 struct page *alloc_migrate_target(struct page *page, unsigned long private,
 				  int **resultp)
diff --git a/mm/swap.c b/mm/swap.c
index 39395fb..678f347 100644
--- a/mm/swap.c
+++ b/mm/swap.c
@@ -896,6 +896,7 @@ void lru_add_drain_all(void)
 	put_online_cpus();
 	mutex_unlock(&lock);
 }
+EXPORT_SYMBOL_GPL(lru_add_drain_all);
 
 /**
  * release_pages - batched page_cache_release()
diff --git a/mm/vmscan.c b/mm/vmscan.c
index 2aec424..d95fa51 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -145,6 +145,7 @@ int vm_swappiness = 60;
  * zones.
  */
 unsigned long vm_total_pages;
+EXPORT_SYMBOL_GPL(vm_total_pages);
 
 static LIST_HEAD(shrinker_list);
 static DECLARE_RWSEM(shrinker_rwsem);
@@ -1444,6 +1445,7 @@ int isolate_lru_page(struct page *page)
 	}
 	return ret;
 }
+EXPORT_SYMBOL_GPL(isolate_lru_page);
 
 /*
  * A direct reclaimer may isolate SWAP_CLUSTER_MAX pages from the LRU list and
@@ -3631,6 +3633,7 @@ void kswapd_stop(int nid)
 		NODE_DATA(nid)->kswapd = NULL;
 	}
 }
+EXPORT_SYMBOL_GPL(kswapd_stop);
 
 static int __init kswapd_init(void)
 {
